{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>1. BUSINESS PROBLEM </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "Build a recommender system to help predict whether someone will enjoy a movie based on past rating information. \n",
    "<br>For this project MovieLens 100k dataset is used which is publicy avaiable for research and analysis purposes\n",
    "\n",
    "https://files.grouplens.org/datasets/movielens/ml-latest-small-README.html\n",
    "</p>    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the necessary libraries\n",
    "from surprise import Reader, Dataset\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from scipy import sparse\n",
    "from scipy.sparse import csr_matrix\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from surprise import Reader, Dataset,SVD\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the data and performing a time based split\n",
    "df = pd.read_csv(r'movielens_data.csv')\n",
    "df = a.sort_values(ascending=True,by=['timestamp'])\n",
    "_80_percent_mark = int(0.80*df.shape[0])\n",
    "train_df = a[0:_80_percent_mark][['user','movie','rating']]\n",
    "test_df = a[_80_percent_mark:][['user','movie','rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating sparse matrix representation our dataframes\n",
    "test_sparse_matrix = sparse.csr_matrix((test_df.rating.values, (test_df.user.values,\n",
    "                                               test_df.movie.values)))\n",
    "\n",
    "train_sparse_matrix = sparse.csr_matrix((train_df.rating.values, (train_df.user.values,\n",
    "                                               train_df.movie.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sparsity(sparse_matrix):\n",
    "    '''returns sparisty of matrix'''\n",
    "    us,mv = sparse_matrix.shape\n",
    "    elem = sparse_matrix.count_nonzero()\n",
    "    sparsity = np.round((1-(elem/(us*mv))) * 100,4)\n",
    "    return sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity of Train matrix : 94.9646 % | Sparsity of Test Matrix : 98.7383 %\n"
     ]
    }
   ],
   "source": [
    "# checking sparsity of train and test matrix\n",
    "print(\"Sparsity of Train matrix : {} % | Sparsity of Test Matrix : {} %\".format(check_sparsity(train_sparse_matrix),check_sparsity(test_sparse_matrix)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AyEJqPka4lBW"
   },
   "outputs": [],
   "source": [
    "def initialize(dim):\n",
    "    '''In this function, we will initialize bias value 'B' and 'C'.'''\n",
    "    # initalize the value to zeros \n",
    "    # return output as a list of zeros \n",
    "    return np.zeros(dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DTDK4ZR18MrZ"
   },
   "source": [
    "<h1>2. RECOMMENDER SYSTEM FROM SCRATCH (COLLABORATIVE FILTERING) </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L = \\min_{ b, c, \\{ u_i \\}_{i=1}^N, \\{ v_j \\}_{j=1}^M}\n",
    "\\quad\n",
    "\\alpha \\Big(\n",
    "    \\sum_{j} \\sum_{k} v_{jk}^2 \n",
    "    + \\sum_{i} \\sum_{k} u_{ik}^2 \n",
    "    + \\sum_{i} b_i^2\n",
    "    + \\sum_{j} c_i^2\n",
    "    \\Big)\n",
    "+ \\sum_{i,j \\in \\mathcal{I}^{\\text{train}}}\n",
    "    (y_{ij} - \\mu - b_i - c_j - u_i^T v_j)^2\n",
    "$$\n",
    "\n",
    "\n",
    "#### We will minimize the above cost function using the below gradient descent algorithm i.e computing gradients w.r.t user and movie biases and learning these biases during the training process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "for each epoch:\n",
    "\n",
    "    for each pair of (user, movie):\n",
    "\n",
    "        b_i =  b_i - learning_rate * dL/db_i\n",
    "\n",
    "        c_j =  c_j - learning_rate * dL/dc_j\n",
    "\n",
    "predict the ratings with formula\n",
    "</pre>\n",
    "\n",
    "$\\hat{y}_{ij} = \\mu + b_i + c_j + \\text{dot_product}(u_i , v_j) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3NFzVC1N8S4L"
   },
   "outputs": [],
   "source": [
    "# methods to compute gradients\n",
    "def derivative_db(user_id,item_id,rating,U,V,mu,alpha):\n",
    "    '''In this function, we will compute dL/db_i'''\n",
    "    loss =  (2*alpha*b_i[user_id]) - 2*(rating - mu - b_i[user_id] - c_j[item_id] - np.dot(U[user_id],V.T[item_id]))\n",
    "    return loss\n",
    "def derivative_dc(user_id,item_id,rating,U,V,mu,alpha):\n",
    "    '''In this function, we will compute dL/dc_j'''\n",
    "    loss =  (2*alpha*c_j[item_id]) - 2*(rating - mu - b_i[user_id] - c_j[item_id] - np.dot(U[user_id],V.T[item_id]))\n",
    "    return loss    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user and movie biases arrays are intialized (size equal to total users,movies in train matrix)\n",
    "dim= train_sparse_matrix.shape[0]\n",
    "b_i=initialize(dim)\n",
    "dim= train_sparse_matrix.shape[1]\n",
    "c_j=initialize(dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FIRST LET US GET A BASELINE RMSE USING GOLBAL AVERAGE RATING TO COMPARE OUR MODELS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_baseline_rmse(train_df,test_df):\n",
    "    mu = train_df['rating'].mean() # computing global average train data\n",
    "    # train rmse for random model \n",
    "    y_true_train = train_df['rating'].tolist()\n",
    "    y_pred_train = [mu]*train_df.shape[0]\n",
    "    train_mse = mean_squared_error(y_true_train,y_pred_train)\n",
    "    # test rmse for random model\n",
    "    y_true_test = test_df['rating'].tolist()\n",
    "    y_pred_test = np.random.randint(1,5,test_df.shape[0])\n",
    "    y_pred_train = [mu]*test_df.shape[0]\n",
    "    test_mse = mean_squared_error(y_true_test,y_pred_test)\n",
    "    print(\"=============================GOLBAL MEAN RATING MODEL=========================\")\n",
    "    print(\"Train RMSE is : {} and Test RMSE : {}\".format(train_mse,test_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================GOLBAL MEAN RATING MODEL=========================\n",
      "Train RMSE is : 1.2709884775 and Test RMSE : 3.6814\n"
     ]
    }
   ],
   "source": [
    "get_baseline_rmse(train_df,test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(df,b_i,c_j,mu):\n",
    "    '''calculates net rmse'''\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for user,movie,rate in df[['user','movie','rating']].values:\n",
    "        try:\n",
    "            y_hat = mu + b_i[user] + c_j[movie] + np.dot(U1[user],V1.T[movie])\n",
    "        except:\n",
    "            # handling cold start problem assigning global average for test users/movies not in training set\n",
    "            y_hat = mu\n",
    "        y_true.append(rate)\n",
    "        y_pred.append(y_hat)\n",
    "    return mean_squared_error(y_true,y_pred)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_recommender(total_epochs,learning_rate,train_sparse_matrix,train_df,test_df,svd_components = 5):\n",
    "    '''learns parameters for the recommednder'''\n",
    "    mu = train_df['rating'].mean() # global average rating in train data\n",
    "    total_train_mse = []\n",
    "    total_test_mse = []\n",
    "    U1, Sigma, V1 = randomized_svd(train_sparse_matrix, n_components=svd_components,n_iter=2, random_state=24)\n",
    "    total_train_mse = []\n",
    "    total_test_mse = []\n",
    "    alpha = 10\n",
    "    for epoch in range(total_epochs):\n",
    "        for user,movie,rate in train_df[['user','movie','rating']].values:\n",
    "            b_i[user] = b_i[user] - learning_rate *  derivative_db(user,movie,rate,U1,V1,mu,alpha) \n",
    "            c_j[movie] = c_j[movie] - learning_rate *  derivative_dc(user,movie,rate,U1,V1,mu,alpha)\n",
    "        train_error = get_prediction(train_df,b_i,c_j,mu)\n",
    "        test_error = get_prediction(test_df,b_i,c_j,mu)\n",
    "        total_train_mse.append(train_error)\n",
    "        total_test_mse.append(test_error)\n",
    "        print(\"After Epoch {}------Train rmse:{}  Test rmse:{}\".format(epoch,train_error,test_error))\n",
    "        print()\n",
    "        print(\"=======================================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the model based recommender system trained to learn user and movie biases, we can clearly see the difference in the test RMSE, hence our model did pick up some useful user/movie specific patterns during the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Epoch 0------Train rmse:1.2709884775  Test rmse:1.2524334124999998\n",
      "\n",
      "=======================================================================================\n",
      "After Epoch 1------Train rmse:1.2709884775  Test rmse:1.2524334124999998\n",
      "\n",
      "=======================================================================================\n",
      "After Epoch 2------Train rmse:1.2709884775  Test rmse:1.2524334124999998\n",
      "\n",
      "=======================================================================================\n",
      "After Epoch 3------Train rmse:1.2709884775  Test rmse:1.2524334124999998\n",
      "\n",
      "=======================================================================================\n",
      "After Epoch 4------Train rmse:1.2709884775  Test rmse:1.2524334124999998\n",
      "\n",
      "=======================================================================================\n"
     ]
    }
   ],
   "source": [
    "fit_recommender(total_epochs = 5, learning_rate = 0.01, \n",
    "                train_sparse_matrix = train_sparse_matrix, \n",
    "                train_df = train_df,test_df=test_df,svd_components = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>3. RECOMMENDER SYSTEM USING SURPRISE LIBRARY </h1>\n",
    "\n",
    "here we are doing the same thing using an already available library called surprise , except now we are learning the user,movie latent vectors along with the user and movie biases during training as well i.e we are performing matrix factorization using our training data, this is more robust and less prone to overfitting. Here is the offical documentation of surprise library to get a better understanding\n",
    "https://surprise.readthedocs.io/en/stable/matrix_factorization.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods to get ratings and compute errors using surprise\n",
    "def get_ratings(predictions):\n",
    "    actual = np.array([pred.r_ui for pred in predictions])\n",
    "    pred = np.array([pred.est for pred in predictions])\n",
    "    return actual, pred\n",
    "\n",
    "def get_errors(predictions, print_them=False):\n",
    "    actual, pred = get_ratings(predictions)\n",
    "    rmse = np.sqrt(np.mean((pred - actual)**2))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(1,5))\n",
    "# create the traindata from the dataframe...\n",
    "train_data = Dataset.load_from_df(train_df[['user', 'movie', 'rating']], reader)\n",
    "# build the trainset from traindata.., It is of dataset format from surprise library..\n",
    "trainset = train_data.build_full_trainset() \n",
    "testset = list(zip(test_df.user.values, test_df.movie.values, test_df.rating.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_surprise(surprise_algo,trainset):\n",
    "    svd.fit(trainset)\n",
    "    train_preds = svd.test(trainset.build_testset())\n",
    "    train_actual_ratings, train_pred_ratings = get_ratings(train_preds)\n",
    "    train_rmse = get_errors(train_preds) \n",
    "    test_preds = svd.test(testset)\n",
    "    test_actual_ratings, test_pred_ratings = get_ratings(test_preds)\n",
    "    test_rmse = get_errors(test_preds)    \n",
    "    print(\"Train rmse : {}  Test rmse : {}\".format(train_rmse,test_rmse))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can clearly see the test rmse improved further, hence learning the user-movie latent vectors as well during training proved to be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing epoch 0\n",
      "Processing epoch 1\n",
      "Processing epoch 2\n",
      "Processing epoch 3\n",
      "Processing epoch 4\n",
      "Train rmse : 0.9319215610883156  Test rmse : 1.0371686201507941\n"
     ]
    }
   ],
   "source": [
    "svd = SVD(n_factors=5, biased=True, random_state=15, verbose=True,n_epochs=5)\n",
    "run_surprise(surprise_algo = svd,trainset = trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>4. SUMMARY and CONCLUSION</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Recommender System Type | TEST RMSE |\n",
    "| --- | --- |\n",
    "| Global Average | 3.68 |\n",
    "|Model-1         |1.25  |\n",
    "|Model-2         |1.03   |  \n",
    "\n",
    "* Here __Model-1__ is the our custom recommender system where we learnt these __user,movie biases__ from the training data. These biases can be thought of as values depicting a user/movie properties for example a customer who is very critical and usually gives lesser ratings as compared to other customers could have a negative user bias like -0.05, similarly some movies are popular and tend to have high rating hence movie biases for some movies would to high therefore these biases help in accomodating these user/movie specific properties and help in refining the predictions, hence Model-1 has better RMSE than a simple average prediction model\n",
    "<br>\n",
    "<br>\n",
    "* In __Model_2__ (trained using surprise) we have also learnt these user,movie latent vectors hence this helped in refining the prediction further by incorporrating the factor of a user-movie interaction in the form of a dot product b/w user movie vectors.\n",
    "<br>\n",
    "<br>\n",
    "* This is just some baseline models,there are still many scope of refinements\n",
    "   * Using a bigger training set\n",
    "   * There are some users/movies in our test data who do not appear in training at all, hence for these users our collabortive model can't say anything since there aren't any past interactions avaialable, this is called a __cold start problem__ and to overcome this we could introduce some additional user/movie like features like age of person, genre of movie,etc basically using a hybrid appraoch by combining Content Based and Collobarative Based approached\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Recommendation_system_assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
